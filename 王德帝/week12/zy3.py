import streamlit as st

from agents.mcp.server import MCPServerSse
import asyncio
from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel, SQLiteSession
from openai.types.responses import ResponseTextDeltaEvent
from agents.mcp import MCPServer
from agents import set_default_openai_api, set_tracing_disabled

set_default_openai_api("chat_completions")
set_tracing_disabled(True)

st.set_page_config(page_title="ä¼ä¸šèŒèƒ½æœºå™¨äºº")
session = SQLiteSession("conversation_123")  # openai agent æä¾›çš„ åŸºäºå†…å­˜çš„ä¸Šä¸‹æ–‡ç¼“å­˜

# streamlit
# session_state å½“å‰å¯¹è¯çš„ç¼“å­˜
# session_state.messages æ­¤æ¬¡å¯¹è¯çš„å†å²ä¸Šä¸‹æ–‡

# é¡µé¢çš„ä¾§è¾¹æ 
with st.sidebar:
    st.title('èŒèƒ½AI+æ™ºèƒ½é—®ç­”')
    if 'API_TOKEN' in st.session_state and len(st.session_state['API_TOKEN']) > 1:
        st.success('API Tokenå·²ç»é…ç½®', icon='âœ…')
        key = st.session_state['API_TOKEN']
    else:
        key = ""

    key = st.text_input('è¾“å…¥Token:', type='password', value=key)

    st.session_state['API_TOKEN'] = key
    model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", ["qwen-flash", "qwen-max"])
    use_tool = st.checkbox("ä½¿ç”¨å·¥å…·")

    # æ˜¾ç¤ºå·¥å…·åˆ—è¡¨
    tool_list_container = st.empty()


# åˆå§‹åŒ–çš„å¯¹è¯
if "messages" not in st.session_state.keys():
    st.session_state.messages = [
        {"role": "system", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä¼ä¸šèŒèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥AIå¯¹è¯ ä¹Ÿ å¯ä»¥è°ƒç”¨å†…éƒ¨å·¥å…·ã€‚"}
    ]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])


def clear_chat_history():
    st.session_state.messages = [
        {"role": "system", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä¼ä¸šèŒèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥AIå¯¹è¯ ä¹Ÿ å¯ä»¥è°ƒç”¨å†…éƒ¨å·¥å…·ã€‚"}
    ]

    global session
    session = SQLiteSession("conversation_123")


st.sidebar.button('æ¸…ç©ºèŠå¤©', on_click=clear_chat_history)

#è·å–å½“å‰å¯ç”¨çš„å·¥å…·åˆ—è¡¨
async def load_tools():
    """
    ä» MCP Server è·å–å½“å‰å¯ç”¨çš„å·¥å…·åˆ—è¡¨
    """
    async with MCPServerSse(
            name="SSE Python Server",
            params={"url": "http://localhost:8900/sse"},
            cache_tools_list=False,
    ) as mcp_server:

        tools = await mcp_server.list_tools()
        #Available tools: ['get_today_daily_news', 'get_douyin_hot_news', 'get_github_hot_news', 'get_toutiao_hot_news', 'get_sports_news', 'get_today_familous_saying', 'get_today_motivation_saying', 'get_today_working_saying', 'get_city_weather', 'get_address_detail', 'get_tel_info', 'get_scenic_info', 'get_flower_info', 'get_rate_transform', 'sentiment_classification', 'summary_work_history']
        return tools

def filter_news_tools(tools):
    """ç­›é€‰æ–°é—»ç±»å·¥å…·"""
    news_tools = ['get_today_daily_news', 'get_douyin_hot_news', 'get_github_hot_news', 
                  'get_toutiao_hot_news', 'get_sports_news']
    return [tool for tool in tools if tool.name in news_tools]

def filter_utility_tools(tools):
    """ç­›é€‰å®ç”¨å·¥å…·ç±»"""
    utility_tools = ['get_city_weather', 'get_address_detail', 'get_tel_info', 
                     'get_scenic_info', 'get_flower_info', 'get_rate_transform', 
                     'sentiment_classification', 'summary_work_history']
    return [tool for tool in tools if tool.name in utility_tools]

async def get_model_response(prompt, model_name, use_tool):
    """
    prompt å½“å‰ç”¨æˆ·è¾“å…¥
    model_name æ¨¡å‹ç‰ˆæœ¬
    use_tool æ˜¯å¦è°ƒç”¨å·¥å…·
    """
    async with MCPServerSse(
            name="SSE Python Server",
            params={
                "url": "http://localhost:8900/sse",
            },
            cache_tools_list=False, # å¦‚æœ True ç¬¬ä¸€æ¬¡è°ƒç”¨åï¼Œç¼“å­˜mcp server æ‰€æœ‰å·¥å…·ä¿¡æ¯ï¼Œä¸å†è¿›è¡Œlist tool
            # tool_filter å¯¹toolç­›é€‰ï¼ˆå¯ä»¥å†™ä¸€ä¸ªå‡½æ•°ç­›é€‰ï¼Œä¹Ÿå¯ä»¥é€šè¿‡é»‘åå•/ç™½åå•ç­›é€‰ï¼‰
            # client_session_timeout_seconds è¶…æ—¶æ—¶é—´
            client_session_timeout_seconds=20,
            tool_filter=lambda tools: filter_news_tools(tools) if "æ–°é—»" in prompt else filter_utility_tools(tools)
    ) as mcp_server:
        external_client = AsyncOpenAI(
            api_key=key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )
        if use_tool:
            agent = Agent(
                name="Assistant",
                instructions="",
                mcp_servers=[mcp_server],
                model=OpenAIChatCompletionsModel(
                    model=model_name,
                    openai_client=external_client,
                )
            )
        else:
            agent = Agent(
                name="Assistant",
                instructions="",
                model=OpenAIChatCompletionsModel(
                    model=model_name,
                    openai_client=external_client,
                )
            )

        # session openai-agent ä¸­ ç¼“å­˜çš„ä¸Šä¸‹æ–‡
        result = Runner.run_streamed(agent, input=prompt, session=session)
        async for event in result.stream_events():
            if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
                yield event.data.delta


if len(key) > 1:
    # ===== æ–°å¢ï¼šä¾§è¾¹æ åŠ è½½å·¥å…·åˆ—è¡¨ =====
    if use_tool:
        try:
            tools = asyncio.run(load_tools())
            tool_list_container.markdown("### ğŸ”§ å¯ç”¨å·¥å…·åˆ—è¡¨")
            tools_info = []  # æ”¶é›†æ‰€æœ‰å·¥å…·ä¿¡æ¯
            for tool in tools:
                tools_info.append(f"""**{tool.name}**""")
            tool_list_container.markdown("\n".join(tools_info))
        except Exception as e:
            tool_list_container.error(f"åŠ è½½å·¥å…·å¤±è´¥ï¼š{e}")

    else:
        tool_list_container.markdown("")  # ä¸å‹¾é€‰æ—¶æ¸…ç©ºæ˜¾ç¤º

    if prompt := st.chat_input():
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"): # ç”¨æˆ·è¾“å…¥
            st.markdown(prompt)

        with st.chat_message("assistant"): # å¤§æ¨¡å‹è¾“å‡º
            message_placeholder = st.empty()
            full_response = ""

            with st.spinner("è¯·æ±‚ä¸­..."):
                try:
                    response_generator = get_model_response(prompt, model_name, use_tool)

                    async def stream_and_accumulate(generator):
                        accumulated_text = ""
                        async for chunk in generator:
                            accumulated_text += chunk
                            message_placeholder.markdown(accumulated_text + "â–Œ")
                        return accumulated_text


                    full_response = asyncio.run(stream_and_accumulate(response_generator))
                    message_placeholder.markdown(full_response)

                except Exception as e:
                    error_message = f"å‘ç”Ÿé”™è¯¯: {e}"
                    message_placeholder.error(error_message)
                    full_response = error_message
                    print(f"Error during streaming: {e}")

            # 4. å°†å®Œæ•´çš„åŠ©æ‰‹å›å¤æ·»åŠ åˆ° session state
            st.session_state.messages.append({"role": "assistant", "content": full_response})
