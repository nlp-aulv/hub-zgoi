作业1、阅读 gpt4rec的论文，总结实施过程。

实施过程：
1. 查询生成（Query Generation）
- 输入：用户历史交互的物品标题序列（最多截断为15个），并按照固定提示模板格式化：
    Previously, the customer has bought: <ITEM TITLE 1>. <ITEM TITLE 2>... In the future, the customer wants to buy
- 模型：使用 GPT-2（117M 参数） 作为生成式语言模型，对上述提示进行微调。
- 训练目标：以用户前 T−1 个物品标题为上下文，预测第 T 个物品的标题（即“最细粒度的查询就是目标物品本身”）。
- 推理策略：采用 多查询束搜索（multi-query beam search），生成多个代表用户不同兴趣维度和粒度的自然语言查询（如生成 5、10、20 或 40 个查询）。
2. 物品检索（Item Retrieval）
- 检索引擎：使用 BM25 搜索算法（一种基于词频和文档长度的经典信息检索方法）。
- 输入：上一步生成的多个自然语言查询。
- 检索方式：
    - 对每个查询，从全量物品库中检索最相关的物品；
    - 采用 分层融合策略：按查询生成得分排序，依次从每个查询结果中取 top-K/m 个不重复的物品（m 为查询数量，K 为目标推荐数量）。
- 优势：
    - 可自然处理冷启动物品（只要有标题即可被检索）；
    - 支持动态更新的物品库（无需重新训练模型）。
3. 训练策略（Two-Step Training）
- 第一步：微调语言模型
    - 使用用户行为序列构造训练样本（前 T−1 个物品 + 最后一个物品标题作为目标）；
    - 用 Adam 优化器训练 GPT-2，学习率 1e-4，2000 步 warm-up，共训练 20 轮。
- 第二步：优化检索参数
- 固定语言模型后，通过网格搜索（grid search）在验证集上优化 BM25 的两个超参：
    - k1 ∈ [0, 3]
    - b ∈ (0, 1)
- 选择使 Recall@K 最高的参数组合。
补充说明
- 灵活性：框架设计模块化，可替换更强大的 LLM（如 GPT-3/4）或更先进的检索系统（如 dense retrieval）。
- 可解释性：生成的查询本身就是对用户兴趣的自然语言解释，便于人工理解和系统调试。
- 多样性控制：通过调节生成查询的数量（beam size）可有效平衡推荐的相关性与多样性。
综上，GPT4Rec 的核心思想是将推荐任务转化为“先生成用户意图查询，再用搜索引擎召回物品”的两阶段流程，兼具性能、可解释性与工程实用性。
